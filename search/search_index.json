{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>LLM Processing Unit(LPU) is latency-optimized and highly scalable hardware architecture that executes large language model (LLM) inference. LPU architecture delivers high performance and energy efficiency compared to other accelerators such as NVIDIA GPU.</p> <p>HyperDex is the end-to-end (E2E) software stack designed specifically for LPU. It enables you to fine-tune your models and fully harness the power of LPU technology. With features that deliver enhanced power efficiency and robust processing capabilities, HyperDex provides the flexibility and speed needed to accelerate your machine learning workflows.</p> <p>HyperDex API is fully compatible with the HuggingFace framework, vLLM, and other custom frameworks. Whether you\u2019re working with HuggingFace LLM models or other models, HyperDex ensures a seamless transition to the LPU. This allows you to focus on innovation, without worrying about infrastructure challenges. </p> <p>HyperDex SDK delivers a complete solution in a single stack, encompassing the compiler, runtime, and drivers, providing a straightforward solution to integrate your pre-trained models into production environments. The unique efficiency of the LPU minimizes the need for extensive manual optimization, enabling faster deployment without compromising on performance.</p> <p>In this docs, we will break down HyperDex into Quick Start, Software Stack, and About LPUs. Please contact to us if you need more help or any assistance. Our team is here to support you every step of the way.</p>"},{"location":"QA_device/","title":"About Device","text":"<p>\ub3d9\uc77c 10B\ubaa8\ub378\uc5d0 \ub300\ud574 LPU / GPU \uc11c\ube59 \ub450 \ud658\uacbd\uc5d0\uc11c \uc0dd\uc131\uacb0\uacfc\uac00 \ucc28\uc774\uac00 \ub9ce\uc774 \ub098\ub294 \ubd80\ubd84\uc774 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>GPU/LPU \ud558\uc774\ube0c\ub9ac\ub4dc \uc11c\ubc84\uc758 \uacbd\uc6b0\uc5d0 GPU only\ub85c \ud588\uc744 \ub54c\uc640 100\ud37c\uc13c\ud2b8 \uc77c\uce58\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud558\ub4dc\uc6e8\uc5b4\ub9c8\ub2e4 \uc0ac\uc6a9\ud558\ub294 \uc5f0\uc0b0\uae30\uac00 \ub2e4\ub974\uace0, \uc5f0\uc0b0\uae30\uc5d0 \ub530\ub77c\uc11c float 16 \ub370\uc774\ud130 \uc911\uc5d0 \ud558\uc704 \ube44\ud2b8 \uc77c\ubd80\uac00 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\u00a0</p> <p>LPU\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \ud2b9\ubcc4\ud788 \uc81c\uc791\ub41c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc57c \ud558\ub098\uc694?</p> <p>\uc800\ud76c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ud1b5\uc6a9\ub418\ub294 huggingface format\uc758 model\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub2e4\ub9cc \uc800\ud76c \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \ub3d9\uc791\uc2dc\ud0a4\uae30 \uc704\ud574\uc11c\ub294 hyperdex_sdk/cli/run.py\ub97c \uc774\uc6a9\ud574 \ubaa8\ub378\uc744 \uc218\uc815\ud558\ub294 \uc57d\uac04\uc758 \ucef4\ud30c\uc77c \uacfc\uc815\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.</p> <p>LPU \uc0c1\ud0dc\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub294 \uba85\ub839\uc5b4\uac00 \uc788\ub098\uc694?</p> <p>\uc790\uccb4 \uac1c\ubc1c\ud55c \uba85\ub839\uc5b4 \ubc0f FPGA \uba85\ub839\uc5b4 \uacf5\uc720 \ub4dc\ub9bd\ub2c8\ub2e4. <code>hyperdex-smi</code>\ub97c \uc0ac\uc6a9\ud558\uc5ec <code>nvidia-smi</code>\uc640 \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c LPU\uc758 \ub3d9\uc791 \uc0c1\ud0dc\ub97c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <code>hyperdex-reset</code>\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub514\ubc14\uc774\uc2a4\uac00 \ub370\ub4dc\ub77d \uc0c1\ud0dc\uc5d0 \ube60\uc84c\uc744 \ub54c LPU\ub97c \ucd08\uae30\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <code>xbutil examine</code>\uc744 \uc0ac\uc6a9\ud558\uc5ec LPU\uac00 \uc798 \uc7a5\ucc29\ub418\uc5b4 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Trouble-Shooting \ud398\uc774\uc9c0\ub97c \ucc38\uace0 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4.</p>"},{"location":"QA_server/","title":"About Server","text":"<p>\uc5e3\uc9c0 \uc11c\ubc84\uc758 \ub3d9\uc2dc\uc811\uc18d\uc790\ub294 \uba87\uba85\uae4c\uc9c0 \uac00\ub2a5\ud55c\uac00\uc694?</p> <p>TPOT, TTFT \ub4f1 \uc138\ubd80 \uc870\uac74\uc5d0 \ub530\ub77c, \ud558\ub4dc\uc6e8\uc5b4\ub97c \uc5b4\ub5bb\uac8c \uc138\ud305\ud558\ub0d0\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9c8 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \ud604\uc7ac \ub300\uc5ec\ud574\ub4dc\ub9b0 edge \uc11c\ubc84\ub974 \uae30\uc900\uc73c\ub85c \ub9d0\uc500\ub4dc\ub9ac\uba74 \uc18c\uc218\uc758 \ub3d9\uc2dc\uc811\uc18d\uc790(\uc57d 4\uba85)\uae4c\uc9c0 \uc9c0\uc6d0 \uac00\ub2a5\ud569\ub2c8\ub2e4. \ucd94\ud6c4 \uc591\uc0b0\ub420 ASIC \uce69\uc740 256\uba85 \uc774\uc0c1\uc758 \uc0ac\uc6a9\uc790\uae4c\uc9c0 \ub3d9\uc2dc\uc5d0 \uc9c0\uc6d0 \uac00\ub2a5\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.</p> <p>\uc5e3\uc9c0\uc11c\ubc84 \ud558\ub4dc\uc6e8\uc5b4 \uc0ac\uc591\uc744 \uc54c\uace0 \uc2f6\uc2b5\ub2c8\ub2e4.</p> <p>\uc7a5\ucc29\ub41c \uac00\uc18d\uae30: U55C x 2, A10 x 1 CPU: Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHz SSD SATA 512GB DRAM: 64GB</p> <p>\ub124\ud2b8\uc6cc\ud06c \uc18d\ub3c4\ub294 \uc5b4\ub290\uc815\ub3c4\ub97c \ud544\uc694\ub85c \ud558\ub098\uc694?</p> <p>\uc800\ud76c \uc11c\ubc84\ub294 \ub124\ud2b8\uc6cc\ud06c \uc5f0\uacb0 \uc5c6\uc774 on-premise\ub85c \ub3d9\uc791 \uac00\ub2a5\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubaa8\ub378\ub9cc \uc788\ub2e4\uba74 \ub124\ud2b8\uc6cc\ud06c \uc5c6\uc774 \ub3d9\uc791 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\uc11c\ubc84\uc5d0 \ubb38\uc81c\uac00 \uc0dd\uacbc\uc744 \ub54c \uae34\uae09\ub300\uc751\uc808\ucc28\ub294 \uc5b4\ub5bb\uac8c \ub418\ub098\uc694?</p> <p>\uace0\uac1d\uc0ac\ub9c8\ub2e4 \uc11c\ubc84\ub2f4\ub2f9 \uad00\ub9ac\uc790\uac00 \ubc30\uc815\ub418\uc5b4 \uc788\uc73c\uba70, \ub2f4\ub2f9\uc790\uc5d0\uac8c \ubb38\uc758\uc8fc\uc2dc\uba74 \ubc14\ub85c \ub2f5\ubcc0 \ubc0f \uae30\uc220\uc9c0\uc6d0 \ub3c4\uc640\ub4dc\ub9ac\uace0 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc9c0\uc6d0\ud558\ub294 OS \uc885\ub958</p> <p>OS\ub294 Centos-7, Ubuntu 22.04 LTS, Rocky 8.4 \uc9c0\uc6d0 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p>"},{"location":"QA_software/","title":"About Software","text":"<p>\uc9c0\uc6d0\uac00\ub2a5\ud55c \uc5b8\uc5b4\ubaa8\ub378 \uc0ac\uc774\uc988\uac00 \uc5b4\ub5bb\uac8c \ub418\ub098\uc694?</p> <p>8LPU\uc778 orion \uc11c\ubc84\uc758 \uacbd\uc6b0 \ucd5c\ub300 66B\uae4c\uc9c0 \uac00\ub2a5\ud569\ub2c8\ub2e4. </p> <p>\uc800\ud76c\uac00 \uae30\uc874\uc5d0 GPU \uae30\ubc18 \uc11c\ube59\uc5d0 \uc0ac\uc6a9\ud558\uace0\uc788\ub294 text generation inference \uc640 \uc720\uc0ac\ud558\uac8c \uc2e4\uc2dc\uac04 \uc694\uccad\uc0ac\ud56d\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 LPU\uae30\ubc18\uc758 \uc11c\ube59 \ud504\ub808\uc784\uc6cc\ud06c\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ud234\uc774 \uc874\uc7ac\ud560\uae4c\uc694?</p> <p>HuggingFace TGI API\ub97c \ub3d9\uc77c\ud558\uac8c \uc0ac\uc6a9\ud558\uace0 \uc788\uc73c\uba70, \uc9c0\ub09c\ubc88 \ub370\ubaa8 \ub54c \ubcf4\uc5ec\ub4dc\ub9b0 \ucc57\ubd07 \ub610\ud55c \ud574\ub2f9 \uc11c\ube59 \uc2a4\ud0dd\uc73c\ub85c \ub3d9\uc791\ud569\ub2c8\ub2e4</p> <p>\ud55c\uad6d\uc5b4\ub3c4 \uc9c0\uc6d0 \uac00\ub2a5\ud55c\uac00\uc694?</p> <p>\ubaa8\ub378\uc774 \ud2b9\uc815 \uc5b8\uc5b4\ub97c \uc9c0\uc6d0\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc740 \ud558\ub4dc\uc6e8\uc5b4\uac00 \uc544\ub2c8\ub77c Training \ubc0f fine-tuning\uc758 \uc601\uc5ed\uc785\ub2c8\ub2e4. \uc800\ud76c LPU \uc5ed\uc2dc \ubaa8\ub378\uc774 \uc9c0\uc6d0\ud558\ub294 \uc5b8\uc5b4\uc640 \ubb34\uad00\ud558\uac8c \uc798 \ub3d9\uc791\ud569\ub2c8\ub2e4. Ko-Alpaca, KULLM3 \ub4f1 \ud55c\uad6d\uc5b4\ub97c \uc798\ud558\ub294 \ubaa8\ub378\uc744 \ub2e4\uc6b4\ubc1b\uc544 \uc2e4\uc81c \uc804\uc2dc\ud68c\uc5d0\uc11c \ud55c\uad6d\uc5b4\ub85c \ub370\ubaa8\ud55c \uacbd\ud5d8\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc9c0\uc6d0 \uac00\ub2a5\ud55c \ubaa8\ub378\uc758 \uc885\ub958</p> <p>Foundation model\uc774 llama\uc778 \ubaa8\ub378\uc740 \ubaa8\ub450 \uc9c0\uc6d0 \uac00\ub2a5\ud569\ub2c8\ub2e4. (ex: llama, llama2, llama3, alpaca, yi \u2026) LPU\uc5d0\uc11c \uc9c0\uc6d0 \uac00\ub2a5\ud55c \ubaa8\ub378 \uc815\ub9ac\ud55c \uc0ac\uc774\ud2b8 \uacf5\uc720\ub4dc\ub9bd\ub2c8\ub2e4. [https://hyper-accel.github.io/docs/models/ ] \uc131\ub2a5\uc740 \ubaa8\ub378\uc5d0 \ub530\ub77c \uc57d\uac04\uc758 \ucc28\uc774\ub294 \uc788\uc73c\ub098 \uac70\uc758 \ubaa8\ub378\uc758 \ud06c\uae30\uc640 \ube44\ub840\ud569\ub2c8\ub2e4.</p> <p>ython. transformer library\uc5d0\uc11c to(\u2019lpu\u2019)\ucc98\ub7fc \ub514\ubc14\uc774\uc2a4\ub97c \uc124\uc815\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218\ub294 \uc5c6\ub098\uc694?</p> <p>\uc800\ud76c\ub294 LPU\ub97c \uc0ac\uc6a9\uc790\uac00 \uc27d\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d python binding\uc744 transformer library\uc640 \uc720\uc0ac\ud558\uac8c \ub9cc\ub4e4\uc5b4 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc989 transformer library \ucf54\ub4dc\uc5d0 \uc800\ud76c \ud558\ub4dc\uc6e8\uc5b4\ub97c \ubd99\uc778 \uac83\uc774 \uc544\ub2c8\ub77c \uc790\uccb4 library\ub97c \ub9cc\ub4e0 \uac83\uc774\uae30\uc5d0 \uc0ac\uc6a9\ubc95\uc774 \uc644\uc804\ud788 \ub3d9\uc77c\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \u201cimport transformer\u201d \ub97c \uc0ac\uc6a9\ud558\uba74 \uae30\uc874\uc5d0 \uc0ac\uc6a9\ud558\uc2dc\ub358 transformer library\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\uace0, \u201cimport hyperdex.transformer\u201d\ub97c \uc0ac\uc6a9\ud558\uba74 \uc800\ud76c\uac00 \uc790\uccb4 \uac1c\ubc1c\ud55c HyperDex Library\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c model\uc744 \ubd80\ub974\uace0 generate\ub97c \uc218\ud589\ud558\ub294 \ubc29\uc2dd\uc740 \uc800\ud76c\uac00 \uc81c\uacf5\ud574\ub4dc\ub9b0 \uc608\uc2dc \ucf54\ub4dc\ub97c \ub530\ub77c\uc8fc\uc154\uc57c \uc815\uc0c1\uc791\ub3d9\ud569\ub2c8\ub2e4. \ud30c\uc774\uc36c \ud568\uc218\uac00 \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud558\ub294\uc9c0 \uad81\uae08\ud558\uc2dc\ub2e4\uba74 /home/server/miniconda3/envs/poc-env/lib/python3.10/site-packages/hyperdex/ \uc5d0 \ub4e4\uc5b4\uac00\uc154\uc11c \uc18c\uc2a4\ucf54\ub4dc\ub97c \ucc38\uace0\ud574\ubcf4\uc154\ub3c4 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.</p> <p>hyperdex\uc5d0\uc11c \uc81c\uacf5\ud558\ub294\u00a0LPU\ub97c \uc0ac\uc6a9\ud558\ub824\uba74,\u00a0\uc5b8\uc5b4\ubaa8\ub378 \ucd08\uae30\ubd80\ud130, hyperdex\uc5d0\uc11c \uc81c\uacf5\ud558\ub294\u00a0SDK library\ub97c \uc774\uc6a9\ud558\uc5ec \uac1c\ubc1c\uc744 \ud574\uc57c\ud558\ub098\uc694?</p> <p>\uc544\ub2d9\ub2c8\ub2e4. Hyperdex\ub294 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uac1c\ubc1c\ud558\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\uac00 \uc544\ub2cc \uc774\ubbf8 \uac1c\ubc1c\ub41c \ubaa8\ub378\uc744\u00a0LPU\ub85c \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac \uc785\ub2c8\ub2e4.\u00a0\ub3c5\ub9bd\uc801\uc73c\ub85c \uac1c\ubc1c\ud558\uc2e0 \ubaa8\ub378\ub3c4\u00a0http://huggingface.co \u00a0\uc5d0 \uc62c\ub77c\uc640\uc788\ub2e4\uba74\u00a0LPU\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\u00a0\uc2e4\uc81c \ub2e4\ub978 \ud68c\uc0ac\uc5d0\uc11c\u00a0fine-tuning\u00a0\ud55c \ubaa8\ub378\uc740 \ubb3c\ub860\u00a0foundation model\uc744 \uac1c\ubc1c\ud55c \ubaa8\ub378\uae4c\uc9c0 \uc218 \ucc28\ub840\u00a0PoC\u00a0\uc9c4\ud589\ud55c \uacbd\ud5d8\uc774 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uae30\uc874 \ucf54\ub4dc(\uc608\ub97c \ub4e4\uc5b4\u00a0transformer\uc744 \uc774\uc6a9\ud55c)\ub97c\u00a0hyperdex.transformer\u00a0\ub85c \ubcc0\uacbd\ud558\uba74, LPU\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub098\uc694?</p> <p>\ub9de\uc2b5\ub2c8\ub2e4.\u00a0\uc774\ubbf8 \uc774\ud574\ud558\uc2e0 \uac83 \uac19\uc9c0\ub9cc \ub2e4\uc2dc \ub9d0\uc500\ub4dc\ub9ac\uba74 \ubc31\uc5d4\ub4dc\ub97c \uc644\uc804\ud558\uac8c \ubd99\uc778 \uac83\uc774 \uc544\ub2c8\ub77c \uae30\uc874 \uc624\ud508\uc18c\uc2a4\ub97c \ubaa8\ubc29\ud55c \uac83\uc774\ubbc0\ub85c \ubb38\ubc95\uc740 \uc800\ud76c \uc608\uc2dc \ucf54\ub4dc\ub97c \ucc38\uace0\ud574\uc8fc\uc154\uc57c \ub354 \uc798 \ub3d9\uc791\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>LPU\ub97c \ud14c\uc2a4\ud2b8\ud558\uae30 \uc704\ud574\uc11c\ub294 hyperdex\u00a0\ub85c\u00a0code\ub97c \ubcc0\ud658\ud558\ub294 \uc791\uc5c5\uc774 \ud544\uc694\ud558\uace0, hyperdex\ub97c \uacf5\ubd80\ud574\uc57c \ud558\ub294 \uacfc\uc815\ub3c4 \ud544\uc694\ud55c\uac00\uc694?</p> <p>\uc800\ud76c\uac00\u00a0python package\ub85c \ud3ec\uc7a5\uc744 \ub2e4 \ud574\ub450\uc5c8\uae30\uc5d0\u00a0generate()\u00a0\ud568\uc218\ub97c \uc2e4\ud589\uc2dc\ud0a4\ub294 \ubd80\ubd84\ub9cc \uc218\uc815\ud574\uc8fc\uc2dc\uba74 \ub3d9\uc791\ud569\ub2c8\ub2e4.\u00a0\ub530\ub77c\uc11c \uba87 \uc904\ub9cc \uac04\ub2e8\ud788 \uc218\uc815\ud558\uba74 \ub2e4\ub978 \ucf54\ub4dc\ub4e4\uacfc \uc798 \ub9de\ubb3c\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\u00a0\ud639\uc2dc \uc5b4\ub824\uc6c0\uc774 \uc788\ub2e4\uba74 \uae30\uc220\uc9c0\uc6d0\uc744 \ucd94\uac00\ub85c \ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>hyperdex\ub97c \uc124\uce58\ud558\ub824\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c \ud558\ub294 \uc9c0 \ud655\uc778\ubd80\ud0c1\ud569\ub2c8\ub2e4.\u00a0pip install hyperdex\u00a0\ud558\ub2c8 \uc548\ub418\ub124\uc694.</p> <p>\ud604\uc7ac\u00a0hyperdex library\ub294 \uc624\ud508 \uc18c\uc2a4\uac00 \uc544\ub2c8\ubbc0\ub85c \uc800\ud76c\uac00\u00a0licensing\u00a0\ud6c4 \ubc30\ud3ec\ud574\ub4dc\ub9ac\uace0 \uc788\uc2b5\ub2c8\ub2e4.\u00a0\ucd94\uac00\uc801\uc778 \uc124\uce58\uac00 \ud544\uc694\ud558\ub2e4\uba74 \uc6d0\uaca9 \uc811\uc18d\uc744 \ud1b5\ud574 \uc124\uce58\ud574\ub4dc\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"chat_ui/","title":"Chat-UI with LPU","text":""},{"location":"chat_ui/#hyperaccel-chatbot-with-orion-server","title":"HyperAccel Chatbot with Orion Server","text":"<p>HyperAccel supports a ChatGPT-style chatbot demo. If you are interested in the demo, please contact us</p> <p></p>"},{"location":"install_guide/","title":"Install guide","text":"<p>Since the LPU is based on Xilinx\u2019s Alveo FPGA, using it requires the installation of Xilinx drivers and the Xilinx Runtime (XRT) software. In addition to these, HyperAccel has developed its own HyperDex Runtime Library (HRT) to efficiently run hardware optimized for large language models (LLM). This runtime allows the LPU to fully utilize its hardware capabilities, providing enhanced performance for deep learning models.</p> <p>Note</p> <p>Since the HyperDex Runtime Library is linked with the Xilinx Runtime (XRT), it is important to install the appropriate versions of both HRT and XRT depending on the Linux kernel version in use. Currently, the HRT supports RHEL-7/8 and Ubuntu-22.04-LTS, ensuring compatibility with these platforms for optimal performance</p> <p>HyperDex Compiler also plays a crucial role in this ecosystem by offering one-click compilation for HuggingFace models, allowing them to seamlessly adapt to the LPU architecture. This enables pre-trained models to be converted into a format optimized for inference on LPU-based hardware with minimal effort.</p> <p>To operate the LPU, all three components (XRT, HRT and Compiler) must be installed on your server. These software stacks are available as both RPM and DEB packages, making them easy to install depending on your operating system.</p> <ul> <li>Xilinx Runtime Library (<code>rpm</code>/<code>deb</code> package)</li> <li>HyperDex Runtime Library (<code>rpm</code>/<code>deb</code> package)</li> <li>HyperDex Compiler Library (<code>rpm</code>/<code>deb</code> package)</li> </ul> <p>Note</p> <p>For the Docker setup we will discuss later, the installation of the XRT is also required. You can find more detailed instructions on how to install XRT through this link.</p> <p>Refer to the step-by-step installation guide provided below. Note that you need an HyperDex portal account to proceed the installation. Please contact us for more information.</p> <p>Warning</p> <p>We are currently in the process of building the HyperDex Portal, but for now, package files are being distributed directly by our team. The installation process after receiving the packages remains the same, so please follow the standard procedure.</p> <ul> <li>Install XRT</li> <li>Install HRT</li> </ul>"},{"location":"install_hrt/","title":"Install hrt","text":"<p>To ensure optimal performance on the LPU, it's essential to install the HyperDex Runtime Library (HRT) alongside the Xilinx Runtime (XRT). The HRT is specifically designed to work with hardware optimized for large language models (LLM), enabling the LPU to maximize its potential in deep learning tasks. The HRT ensures smooth communication between software and hardware and enhances performance for AI workloads.</p> <p>Note</p> <p>Before installing HRT, you need to ensure that the Xilinx Runtime (XRT) is already installed, as HRT depends on XRT for its operations. Make sure that the versions of both HRT and XRT are compatible with your Linux kernel version, as HRT currently supports Centos-7, Ubuntu 22.04 LTS, Rocky 8.4.</p>"},{"location":"install_hrt/#step-1-install-hrt-library","title":"Step 1. Install HRT Library","text":"<p>For now, the HyperDex Portal is under development, but you can still get the HRT package directly from our team. Please contact us to receive the package files.</p> <p>Note</p> <p>The HRT package is available in two versions CPU-only version and CUDA version. You can choose the one that best suits your hardware and needs.</p> <p>For Each-based systems (e.g. Centos-7, Ubuntu 22.04 LTS, Rocky 8.4) <pre><code># yum install hyperdex-hrt_1.3.2_cpu.rpm or hyperdex-hrt_1.3.2_cu.rpm\n</code></pre> <pre><code># dpkg -i hyperdex-hrt_1.3.2_cpu_amd64.deb or hyperdex-hrt_1.3.2_cu_amd64.deb\n</code></pre> Once the installation is complete, the HRT files will be located under the <code>/opt/hyperdex/hrt</code> directory. You will need to source the <code>/opt/hyperdex/setup.sh</code> script to use commands such as <code>hyperdex-smi</code>, <code>hyperdex-reset</code>, <code>hyperdex-net</code>.</p> <p>How to use HyperDex Commands</p> <p><code>hyperdex-smi</code>: This command is used for monitoring the status of HyperDex devices. It provides information about the state and health of your hardware.</p> <p><code>hyperdex-reset</code>: This command resets HyperDex devices to their default state. Use this command to resolve issues or to reset the devices for reconfiguration.</p> <p><code>hyperdex-net</code>: This command is used for managing network configurations between multiple FPGAs. It helps generate the table.json file, which defines the communication paths between devices, based on the network.xclbin file.</p>"},{"location":"install_hrt/#step-2-connect-qsfp-cables","title":"Step 2. Connect QSFP Cables","text":"<p>Before generating the <code>table.json</code> file, ensure that the QSFP cables are correctly connected to each device in a ring topology. This setup allows network communication between multiple FPGAs. For multi-FPGA configurations, each device should be connected to its neighboring FPGA in the network. </p> <p>Ring Topology: Connect the QSFP cable from the last FPGA back to the first FPGA to complete the loop.</p> <p>Warning</p> <p>Proper cable connections are essential for the FPGAs to communicate, and without this step, the subsequent network configuration may fail.</p>"},{"location":"install_hrt/#step-3-generate-tablejson-file","title":"Step 3. Generate table.json File","text":"<p>After connecting the QSFP cables and installing the HRT package, the next step is to generate the table.json file, which is essential if you're working in a multi-FPGA environment. This file defines the network communication paths between the FPGAs. If you're using a single FPGA, this file may not be required unless your application specifically needs network configuration.</p> <p>You can use the <code>hyperdex-net</code> command to generate and manage the <code>table.json</code> file. When you run <code>hyperdex-net</code>, the <code>table.json</code> file will be generated in the <code>/opt/hyperdex/hrt/xclbin</code> directory. If all the values in the generated table are <code>0</code>, this indicates that the table was not created correctly and you may need to troubleshoot the configuration. .</p> <pre><code># ./hyperdex-net\n</code></pre> <pre><code># vi /opt/hyperdex/hrt/xclbin/table.json\n\n{\n    \"network\" :\n    [\n        {\n            \"direction\" : \"Right\",\n            \"dst_id\" : 1,\n            \"src_id\" : 0\n        },\n        {\n            \"direction\" : \"Right\",\n            \"dst_id\" : 2,\n            \"src_id\" : 1\n        },\n        {\n            \"direction\" : \"Right\",\n            \"dst_id\" : 3,\n            \"src_id\" : 2\n        },\n        {\n            \"direction\" : \"Right\",\n            \"dst_id\" : 0,\n            \"src_id\" : 3\n        }\n         ],\n    \"num_device\" : 4\n</code></pre>"},{"location":"install_hyperdex/","title":"HyperDex Installation","text":"<p>Since the LPU is based on AMD\u2019s Alveo FPGA, using it requires the installation of Xilinx drivers and the Xilinx Runtime (XRT) software. In addition to these, HyperAccel has developed its own HyperDex Runtime Library (HRT) to run hardware optimized for large language models (LLM) efficiently. This runtime allows the LPU to fully utilize its hardware capabilities, providing enhanced performance for deep learning models.</p>"},{"location":"install_hyperdex/#step-1-requirements","title":"STEP 1: Requirements","text":"<p>Currently, the HRT supports RHEL-8/8 and Ubuntu-22.04-LTS, ensuring compatibility with these platforms for optimal performance. Please follow XRT install guide.</p> <p>The table below shows the compatibility of Python, CUDA, and Torch versions for the HyperDex package. Please ensure your environment matches one of the supported configurations before installation.</p> Python Version CUDA Version Torch Version 3.9 12.1, 12.4 2.1.0, 2.4.0 3.10 12.1, 12.4 2.1.0, 2.4.0 3.11 12.1, 12.4 2.1.0, 2.4.0 3.12 12.1 2.4.0"},{"location":"install_hyperdex/#notes","title":"Notes:","text":"<ul> <li>Python 3.12:</li> <li>Torch 2.1.0 is not supported.</li> <li>CUDA 12.4 is not compatible.</li> <li>CUDA Compatibility:</li> <li>Torch 2.1.0 does not support CUDA 12.4.</li> <li>Torch 2.4.0 supports both CUDA 12.1 and 12.4.</li> </ul>"},{"location":"install_hyperdex/#step-2-install-python-package","title":"STEP 2: Install Python Package","text":"<p>You can install <code>hyperdex python package</code> using pip, which requires access rights to HyperAccel's private PyPI server. To install the HyperDex Python package, run the following command:</p> <pre><code>$ # (Recommended) Create a new conda environemnt.\n$ conda create -n hdex-env python=3.10 -y\n$ conda activate hdex-env\n\n$ # Install HyperDex Python Package\n$ pip install -i https://pypi.hyperaccel.ai/simple hyperdex-transformers\n$ pip install -i https://pypi.hyperaccel.ai/simple hyperdex-compiler\n</code></pre>"},{"location":"install_hyperdex/#step-3-install-device-package","title":"STEP 3: Install Device Package","text":"<p>For security issues, we are directly providing it to those who have inquired.</p>"},{"location":"install_hyperdex/#step-4-quick-start","title":"STEP 4: Quick Start","text":"<p>To operate the LPU, all three components (XRT, Python Package and RPM Package) must be installed on your server. If you are ready, proceed to the next step.</p> <p>Refer to the step-by-step installation guide provided below. Note that you need an HyperDex portal account to proceed the installation. Please contact us for more information.</p>"},{"location":"install_xrt/","title":"Install xrt","text":"<p>To use the LPU effectively, installing the Xilinx Runtime (XRT) software is essential. XRT ensures smooth integration with Xilinx's Alveo FPGA, optimizing software-hardware communication for peak performance. It also supports the HyperDex Runtime Library (HRT) and ensures compatibility with Centos-7, Ubuntu 22.04 LTS, Rocky 8.4.</p> <p>Before starting the installation, make sure that the LPU is properly connected to the device. Then, refer to the step-by-step installation guide provided below. To proceed, you will need to download the necessary packages from the Xilinx website.</p>"},{"location":"install_xrt/#step-1-install-xrt-library","title":"Step 1. Install XRT Library","text":"<p>Note</p> <p>To install XRT, ensure that the kernel version and kernel-headers version match. Additionally, if your operating system is Ubuntu and the kernel version is 5.15.0-41-generic or higher, you will need to downgrade the kernel. The recommended kernel version is 5.15.0-25.</p> <p>For Each-based systems (e.g. Centos-7, Ubuntu 22.04 LTS, Rocky 8.4) <pre><code># yum install xrt_202310.2.15.225_8.1.1911-x86_64-xrt.rpm\n</code></pre> <pre><code># dpkg -i xrt_202310.2.15.225_22.04-amd64-xrt.deb\n</code></pre> Once the installation is complete, the XRT files will be located under the <code>/opt/xilinx/xrt/</code> directory. You will need to source the <code>/opt/xilinx/xrt/setup.sh</code> script to use commands such as <code>xbutil</code> and <code>xbmgmt</code>.</p> <pre><code>$ source /opt/xilinx/xrt/setup.sh\n</code></pre> How to use XRT Commands xbutil : Used primarily for device status monitoring and diagnostics. xbmgmt : Focuses on firmware management, flashing, and hardware-related tasks.  You can use <code>xbutil</code> and <code>xbmgmt</code> commands to examine device information. For more details, you can use the <code>--help</code> option with these commands or refer to the documentation on the XRT Master documentation.  <pre><code>$ xbutil examine\nSystem Configuration\n  OS Name              : Linux\n  Release              : 5.15.0-25-generic\n  Version              : #25-Ubuntu SMP Wed Mar 30 15:54:22 UTC 2022\n  Machine              : x86_64\n  CPU Cores            : 48\n  Memory               : 257574 MB\n  Distribution         : Ubuntu 22.04.4 LTS\n  GLIBC                : 2.35\n  Model                : ESC4000-E10\n\nXRT\n  Version              : 2.15.225\n  Branch               : 2023.1\n  Hash                 : adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  Hash Date            : 2023-05-03 10:13:19\n  XOCL                 : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  XCLMGMT              : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n\nDevices present\nBDF             :  Shell                            Platform UUID        Device ID Device Ready*\n------------------------------------------------------------------------------------------------\n[0000:17:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n[0000:18:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n[0000:31:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n[0000:32:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n</code></pre> <pre><code>$ xbmgmt examine\nSystem Configuration\n  OS Name              : Linux\n  Release              : 5.15.0-25-generic\n  Version              : #25-Ubuntu SMP Wed Mar 30 15:54:22 UTC 2022\n  Machine              : x86_64\n  CPU Cores            : 48\n  Memory               : 257574 MB\n  Distribution         : Ubuntu 22.04.4 LTS\n  GLIBC                : 2.35\n  Model                : ESC4000-E10\n\nXRT\n  Version              : 2.15.225\n  Branch               : 2023.1\n  Hash                 : adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  Hash Date            : 2023-05-03 10:13:19\n  XOCL                 : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  XCLMGMT              : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n\nDevices present\nBDF             :  Shell                            Platform UUID        Device ID Device Ready*\n------------------------------------------------------------------------------------------------\n[0000:17:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n[0000:18:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n[0000:31:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n[0000:32:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n</code></pre>"},{"location":"install_xrt/#step-2-install-xrt-firmware","title":"Step 2. Install XRT Firmware","text":"<p>The next step is to install the XRT Firmware, which enables the FPGA to handle both hardware acceleration and general-purpose processing. To complete the installation, you need to flash the firmware onto the FPGA card. Follow the instructions provided with the firmware package and use the specified shell commands to deploy it onto the card.</p> <p>Download the Deployment Target Platform: This is the communication layer physically implemented and flashed into the card.</p> <p>For Each-based systems (e.g. Centos-7, Ubuntu 22.04 LTS, Rocky 8.4 <pre><code># tar -xvf xilinx-u55c-gen3x16-xdma_2023.1_2023_0507_2220-noarch.rpm.tar.gz\n# yum install xilinx-*.rpm\n</code></pre> <pre><code># tar -xvf xilinx-u55c-gen3x16-xdma_2023.1_2023_0507_2220-all.deb.tar.gz\n# dpkg -i  xilinx-*.deb\n</code></pre></p> <p>Flash the Firmware: Execute the following command in the shell to flash the firmware onto the FPGA card: <pre><code># xbmgmt program --base --device &lt;BDF&gt; --image xilinx_u55c_gen3x16_xdma_base_3\n</code></pre></p>"},{"location":"install_xrt/#step-3-cold-reboot","title":"Step 3. Cold Reboot","text":"<p>A cold reboot is required after installing the Xilinx firmware to ensure the system initializes and applies the updated firmware to the FPGA hardware.</p> <p>The FPGA may appear as two logical devices. This happens because the FPGA is operating in dual mode, with each logical device assigned to handle different tasks, such as hardware acceleration and general-purpose processing.</p> <p>After applying the shell, the Device Ready status is marked as \"Yes\". <pre><code>Devices present\nBDF             :  Shell                            Platform UUID        Device ID Device Ready*\n------------------------------------------------------------------------------------------------\n[0000:17:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------  user(inst=---)  Yes\n[0000:18:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------  user(inst=---)  Yes\n</code></pre></p>"},{"location":"model_compile/","title":"Model Compilation","text":"<p>HyperDex-Compiler SDK helps to run LPUs using Hugging Face's checkpoints. The following content explains how to transform HuggingFace models to be executable on LPUs.</p>"},{"location":"model_compile/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS, Rocky 8.4</li> <li>Python: 3.9 ~ 3.11</li> <li>Xilinx Runtime Library</li> <li>HyperDex Runtime &amp; Compiler stack</li> </ul>"},{"location":"model_compile/#install-with-pip","title":"Install with pip","text":"<p>You can install <code>hyperdex-compiler</code> using pip, which requires access rights to HyperAccel's private PyPI server. To install the HyperDex Python package, run the following command:</p> <pre><code>$ # (Recommended) Create a new conda environemnt.\n$ conda create -n hdex-env python=3.10 -y\n$ conda activate hdex-env\n\n$ # Install HyperDex-Python and HyperDex-Compiler SDK\n$ pip install -i https://pypi.hyperaccel.ai/simple hyperdex-transformers\n$ pip install -i https://pypi.hyperaccel.ai/simple hyperdex-compilers\n</code></pre>"},{"location":"model_compile/#compile-huggingface-hub-model","title":"Compile HuggingFace-Hub Model","text":"<p>HyperDex-Compiler SDK provides a CLI-based program that converts and compile HyperDex supported models. To refer to the list of models supported by the HyperDex SDK, please visit the following page: Supported Models. Any model in Huggingface that matches the architecture can be converted and run by HyperAccel LPU\u2122. You can easily compile models as shown in the exmaple below using 'AutoCompiler'.</p> <p>Start the SDK program: <pre><code>from hyperdex.tools import AutoCompiler\nmodel_id = 'facebook/opt-1.3b'\nhdex = AutoCompiler()\nhdex.compile(\n    model_id=model_id,\n    num_device=lpu_device,\n    max_length=4096,\n    low_memory_usage=True,\n    token=HF_TOKEN,\n)\n</code></pre> The model_id can be a id from Huggingface(i.g. facebook/opt-1.3b). The model will be installed in ~/.cache/hyperdex/hub. Another option is to spciefy a directory to install the model. Note that the Huggingface model id has be at the end of the directory. HyperDex SDK will download, convert, and save files to run the LPU at the directory described above. <pre><code>model_id = '/opt/hyperdex/models/facebook/opt-1.3b'\n</code></pre></p> <p>Here is an example of running the SDK program for the <code>facebook/opt-1.3b</code> model: <pre><code>[ INFO ] : ** HyperDex  Model  SDK  Version **\n[ INFO ] : ** Copyright (c) 2024 HyperAccel **\n[ INFO ] : ** Phase-0 : Check supported model **\n[ INFO ] : In this phase, check if given model is supported by HyperDex.\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 653/653\n[ INFO ] : ** Phase-1 : Download model from HuggingFace Hub **\n[ INFO ] : In this phase, download the HuggingFace model and save it to (=/home/members/user/.cache/hyperdex/hub).\n[ INFO ] : Download model at /home/members/user/.cache/hyperdex/hub/facebook/opt-1.3b/ckpt\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.63G/2.63G\ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 137/137\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 685/685\nvocab.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 899k/899k\nmerges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441\nNot logged in!\n[ INFO ] : ** Phase-2 : Convert the checkpoint format to HyperDex style **\n[ INFO ] : In this phase, convert the Huggingface checkpoint into HyperDex format.\n[ INFO ] : The output creates both JSON and binary file.\n[ INFO ] : Convert the model to HyperDex model format\n[ INFO ] : Save the converted checkpoint at /home/members/user/.cache/hyperdex/hub/facebook/opt-1.3b/ckpt\n[ INFO ] : ** Phase-3 : Generate Optimized Model Mapping **\n[ INFO ] : In this phase, optimize model mapping for streamlined memory access and efficient model parallelism.\n[ INFO ] : The result of the optimization process is a binary file.\n[ INFO ] : Optimize the model paramater\n[ INFO ] : Save the optimized data at /home/members/user/.cache/hyperdex/hub/facebook/opt-1.3b/param\n[ INFO ] : ** Phase-4 : Generate Optimized Model Instruction **\n[ INFO ] : In this phase, analyze the structure of the model to generate optimized instructions.\n[ INFO ] : Techniques like Kernel Fusion and Layer Reordering techniques are used to make hardware processing more efficient.\n[ INFO ] : The result of the optimization is a binary file.\n[ INFO ] : Optimize the model instruction\n[ INFO ] : Save the optimized instruction at /home/members/user/.cache/hyperdex/hub/facebook/opt-1.3b/inst\n[ INFO ] : Model compile Complete!\n</code></pre></p>"},{"location":"model_compile/#autocompilercompile-parameters","title":"AutoCompiler.compile( ) Parameters","text":"Arguments Description <code>model_id</code> Huggingface model id. Default value is <code>facebook/opt-1.3b</code> <code>token</code> Huggingface Acess Token. Default value is <code>None</code> <code>num_device</code> Number of LPU to use. Default value is <code>1</code> <code>type_device</code> Type of FPGA to use. Default value is <code>u55c</code> of AMD. Further types will be supported <code>max_length</code> Maximum length of response. Defualt value is <code>1</code> <code>low_memory_usage</code> Remove bin/safetensors file to save memory. Default value is <code>False</code>(Doesnt' remove) <code>fast_compile</code> Compile missing files for faster use. Default value is <code>False</code>(Compile every file) <p>If the model is gated in huggingface like mistralai/Mistral-7B-v0.1 or meta-llama/Llama-2-7b-chat-hf, user has to get access via huggingface before using. After getting access from the author, HyperDex SDK will download the model. Access token can be passed either by parameter or after running the SDK program like below. <pre><code>    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nEnter your token (input will not be visible):\n</code></pre></p> <p>Warning</p> <p>When trying to compile model for LPU-GPU Hybrid System, the <code>low_memory_usage</code> must be False. This causes error when GPU is running. If a model was compiled with <code>low_memory_usage</code>=True, compile again with <code>low_memory_usage</code>=False.</p>"},{"location":"python_api/","title":"Python API","text":"<p>HyperDex provides a Python API designed to make running workloads on the LPU both easy and efficient. The Python API uses function calls similar to those found in HuggingFace\u2019s transformers library, allowing users familiar with HuggingFace to quickly adapt to and utilize the HyperDex system. This enables existing HuggingFace users to seamlessly integrate HyperDex into their workflows without a steep learning curve.\u200b</p>"},{"location":"python_api/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS, Rocky 8.4</li> <li>Python: 3.9 ~ 3.11</li> <li>Xilinx Runtime Library</li> <li>HyperDex Runtime &amp; Compiler stack</li> </ul>"},{"location":"python_api/#install-with-pip","title":"Install with pip","text":"<p>You can install <code>hyperdex-transformers</code> using pip, which requires access rights to HyperAccel's private PyPI server. To install the HyperDex Python package, run the following command:</p> <pre><code>$ # (Recommended) Create a new conda environemnt.\n$ conda create -n hdex-env python=3.10 -y\n$ conda activate hdex-env\n\n$ # Install HyperDex-Python\n$ pip install -i https://pypi.hyperaccel.ai/simple hyperdex-transformers\n</code></pre>"},{"location":"python_api/#text-generation-with-hyperaccel-lputm","title":"Text Generation with HyperAccel LPU\u2122","text":"<p>HyperDex allows you to generate output tokens using a function similar to HuggingFace's <code>generate</code> function. Therefore, you can easily generate tokens as shown in the example below.</p> <pre><code># Immport HyperDex transformers\nfrom hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\n\n# Path to hyperdex checkpoint (MODIFY model path and model here)\nckpt = \"/path/to/llama-7b\"\n\n# Load tokenzier and model (MODIFY hardware configuration here)\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForCausalLM.from_pretrained(ckpt, device_map={\"lpu\": 1})\n\n# Input text (MODIFY your input here)\ninputs = \"Hello world!\"\n\n# 1. Encode input text to input token ids\ninput_ids = tokenizer.encode(inputs, return_tensors=\"np\")\n# 2. Generate output token ids with LPU\u2122 (MODIFY sampling parameters here)\noutput_ids = model.generate(\n  inputs,\n  max_length=1024,\n  # Sampling\n  do_sample=True,\n  top_p=0.7,\n  top_k=50,\n  temperature=1.0,\n  repetition_penalty=1.2,\n  # Stopping\n  early_stopping=False\n)\n# 3. Decode output token ids to output text\noutputs = tokenizer.decode(output_ids, skip_special_tokens=True)\n\n# Print the output context\nprint(outputs)\n</code></pre>"},{"location":"python_api/#lpu-gpu-hybrid-system","title":"LPU-GPU Hybrid System","text":"<p>Starting from version 1.3.2, HyperDex-Python supports the LPU-GPU hybrid system. The GPU, which has relatively higher computing power, handles the Prefill part of the Transformer, while the LPU, which efficiently utilizes memory bandwidth, processes the Decode part. The Key-Value transfer between Prefill and Decode can be performed without overhead using HyperDex's proprietary technology. You can select the number of devices to use for both GPU and LPU through the <code>device_map</code> option.</p> <p>Note</p> <p>To run the <code>LPU-GPU hybrid system</code>, you need to have <code>CUDA 12.1</code> installed on your system. Additionally, since the GPU utilizes <code>PyTorch</code> to run LLMs, it is recommended to install <code>torch version 2.4.0 or later</code> to ensure optimal compatibility and performance.\u200b</p> <pre><code># Immport HyperDex transformers\nfrom hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\n\n# Path to hyperdex checkpoint (MODIFY model path and model here)\nckpt = \"/path/to/llama-7b\"\n\n# Use LPU-GPU hybrid system with devce_map option\ntokenizer = AutoTokenizer.from_pretrained(model_id=hyperdex_ckpt)\nmodel = AutoModelForCausalLM.from_pretrained(model_id=hyperdex_ckpt, device_map={\"gpu\": 1, \"lpu\": 1})\n</code></pre> <p>To use the hybrid system, you need CUDA version 12.1 or later and the corresponding version of PyTorch.</p>"},{"location":"python_api/#sampling","title":"Sampling","text":"<p>Sampling works in the same way as HuggingFace. For sampling, you have options like top_p, top_k, temperature, and repetition penalty. Please refer to the HuggingFace documentation for explanations of each option. Additionally, the <code>generate</code> function allows you to directly control randomness using the seed argument. If <code>do_smaple</code> is <code>False</code>, LPU does not perform sampling and uses greedy method.</p> Sampling Arguments Description <code>top_p</code> Top-P sampling. Default value is <code>0.7</code> <code>top_k</code> Top-K sampling. Default value is <code>1</code> <code>temperature</code> Smoothing the logit distribution. Defualt value is <code>1.0</code> <code>repetition_penalty</code> Give penlaty to logits. Default value is <code>1.2</code> <code>stop</code> Token ID that signals the end of generation. Default value is <code>eos_token_id</code>"},{"location":"python_api/#streaming-token-generation","title":"Streaming Token Generation","text":"<p>HyperDex supports streaming token generation in a similar manner to HuggingFace. You can activate it by passing the TextStreamer module as an argument to the <code>generate</code> function.</p> <pre><code># Import HyperDex transformers\nfrom hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\nfrom hyperdex.transformers import TextStreamer\n\n# Path to hyperdex checkpoint\nhyperdex_ckpt = \"/path/to/llama-7b\"\n\n# Load tokenzier and model\ntokenizer = AutoTokenizer.from_pretrained(model_id=hyperdex_ckpt)\nmodel = AutoModelForCausalLM.from_pretrained(model_id=hyperdex_ckpt, device_map={\"lpu\": 1})\n# Config streamer module\nstreamer = TextStreamer(tokenizer, skip_special_tokens=True)\n</code></pre> <p>Since the <code>TextStreamer</code> module includes the process of decoding through the <code>tokenizer</code> and printing internally, you only need to call the <code>generate</code> function.</p> <pre><code># Input text\ninputs = \"Hello world!\"\n\n# 1. Encode input text to input token ids\ninput_ids = tokenizer.encode(\"Hello world!\", return_tensors=\"np\")\n# 2. Generate streaming output token ids with LPU\u2122\nmodel.generate(\n  inputs,\n  max_length=1024,\n  # Sampling\n  do_sample=True,\n  top_p=0.7,\n  top_k=50,\n  temperature=1.0,\n  repetition_penalty=1.2,\n  # Streaming\n  streamer=streamer\n)\n</code></pre>"},{"location":"python_api/#how-to-use-streaming-with-other-application","title":"How to use streaming with other Application?","text":"<p>HyperDex utilizes the <code>yield</code> keyword in Python to enable streaming for use in other applications. When you call the <code>generate_yield</code> function, it returns using <code>yield</code>, making it easy to use in other Python applications.</p> <pre><code># Config streamer module with disable print to activate yield mode\nstreamer = TextStreamer(tokenizer, use_print=False, skip_special_tokens=True)\n\n# Input text\ninputs = \"Hello world!\"\n\n# 1. Encode input text to input token ids\ninput_ids = tokenizer.encode(\"Hello world!\", return_tensors=\"np\")\n# 2. Generate streaming output token ids with LPU\u2122\noutput_ids = model.generate_yield(\n  inputs,\n  max_length=1024,\n  # Sampling\n  do_sample=True,\n  top_p=0.7,\n  top_k=50,\n  temperature=1.0,\n  repetition_penalty=1.2,\n  # Streaming\n  streamer=streamer\n)\n\n# Use outputs_ids which type is generator\n</code></pre>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#text-generation-with-hyperaccel-lputm","title":"Text Generation with HyperAccel LPU\u2122","text":"<p>Similar to HuggingFace transformer package, HyperDex uses an <code>AutoModelForCausalLM</code> module to load the Transformers. To load the model parameters, you can simply give the path of the HyperDex model checkpoint.</p> <pre><code>from hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\n\n# Load tokenzier and model\ntokenizer = AutoTokenizer.from_pretrained(\"llama-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"llama-7b\", device_map={\"lpu\": 1})\n\n# Text Generation\ninput_ids = tokenizer.encode(\"Hello world!\", return_tensors=\"np\")\noutput_ids = model.generate(input_ids, max_length=1024, do_sample=False)\noutputs = tokenizer.decode(input_ids)\n</code></pre> <p>The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the <code>generate</code> API.</p> <p>Note</p> <p>To run the above steps, you must first install the <code>hyperdex-python</code> package using pip. For detailed instructions on the installation process, please refer to Python API page of the documentation.\u200b</p>"},{"location":"quick_start/#main-features","title":"Main features","text":"<ul> <li>APIs of <code>hyperdex.transformers</code> are similar to HuggingFace, which are easy to integrate with various LLM applications.</li> <li>Fast model loading scheme with custom checkpoint format</li> <li>Streaming text generation</li> </ul>"},{"location":"quick_start/#quick-guide-pdf","title":"Quick Guide (PDF)","text":"<p>If you have a server with HyperDex installed, please refer this PPT.</p>"},{"location":"supported_models/","title":"Supported Models","text":"<p>HyperDex supports a variety of generative Transformer models in HuggingFace Transformers. The following is the list of model architectures that are currently supported by HyperDex. Alongside each architecture, we include some popular models that use it.</p> Architecture Models Example HuggingFace Models Hybrid <code>CohereForCausalLM</code> Cohere <code>CohereForAI/c4ai-command-r-v01</code>, etc. <code>ExaoneForCausalLM</code> EXAONE <code>LGAI-EXAONE/EXAONE-3.0-7.8B</code>, etc. <code>FalconForCausalLM</code> Falcon <code>tiiuae/falcon-7b</code>, <code>tiiuae/falcon-40b</code>, etc. <code>GemmaForCausalLM</code> Gemma <code>google/gemma-2b</code>, <code>google/gemma-7b</code>, etc. <code>GPT2LMHeadModel</code> GPT-2 <code>gpt2</code>, <code>gpt2-xl</code>, etc. <code>GPTBigCodeForCausalLM</code> StarCoder, SantaCoder, WizardCoder <code>bidcode/starcoder</code>, etc. <code>GPTJForCausalLM</code> GPT-J <code>EleutherAI/gpt-j-6b</code>, etc. <code>GPTNeoXForCausalLM</code> GPT-NeoX,Pythia,StableLM <code>EleutherAI/gpt-neox-20b</code>, <code>EleutherAI/pythia-12b</code>, etc. <code>InternLM2ForCausalLM</code> InternLM2 <code>internlm/internlm2-7b</code>, etc. <code>LlamaForCausalLM</code> LLaMA,LLAMA-2,LLAMA-3 <code>meta-llama/Llama-2-7b-hf</code>, <code>01-ai/Yi-6B</code>, etc. <code>MistralForCausalLM</code> MistralMistral-Instruct <code>mistralai/Mistral-7B-v0.1</code>, etc. <code>OPTForCausalLM</code> OPT <code>facebook/opt-1.3b</code>, <code>facebook/opt-66b</code>, etc. <code>OlmoForCausalLM</code> OLMo <code>allenai/OLMo-7B</code>, etc. <code>OrionForCausalLM</code> Orion <code>OrionStarAI/Orion-14B-Base</code>, etc. <code>PhiForCausalLM</code> Phi <code>microsoft/phi-1_5</code>, <code>microsoft/phi-2</code>, etc. <code>Phi3ForCausalLM</code> Phi3 <code>microsoft/phi-3</code>, etc. <code>Qwen2ForCausalLM</code> Qwen2 <code>Qwen/Qwen2-beta-7B</code>, <code>Qwen/Qwen2-beta-7B-Chat</code>, etc. <code>StableLmForCausalLM</code> StableLM <code>stabilityai/stablelm-3b-4e1t/</code>, etc. <code>StarCoder2ForCausalLM</code> StarCoder2 <code>bigcode/starcoder2-15b</code>, etc. <p>Note</p> <p>Models marked under the \u201cHybrid\u201d tab have been validated for use in the LPU-GPU Hybrid system. For GPU validation, the models have been tested with NVIDIA\u2019s Ampere, Hopper, and Ada Lovelace product lines, ensuring compatibility and performance across these architectures.</p>"},{"location":"tgi_serve/","title":"TGI Server","text":"<p>HyperDex SDK provides the HyperDex-Serve Python package for serving LLMs. HyperDex-Serve allows the LPU (or LPU-GPU Hybrid System) to be run via a RESTful API. The API follows the same structure as HuggingFace\u2019s widely-used Text-Generation-Inference (TGI) API, which is commonly used for serving open-source LLMs.</p>"},{"location":"tgi_serve/#requirements-and-install-guide","title":"Requirements and Install Guide","text":"<p>Requirements and Install Guide is same as Python API. </p>"},{"location":"tgi_serve/#serving-model","title":"Serving Model","text":"<p>hyperdex-serve provides an HTTP server that implements TGI API.  You can execute the server by the command below.</p>"},{"location":"tgi_serve/#usage","title":"Usage","text":""},{"location":"tgi_serve/#command-line-interface","title":"Command Line Interface","text":"<p>Once the HyperDex-Serve package is installed, you can use the <code>hdex-serve</code> command with the following options:</p> <pre><code>$ hdex-serve --help\nusage: hdex-serve [-h] [--host HOST] [--port PORT]\n                  [--allow-credentials] [--allowed-origins ALLOWED_ORIGINS]\n                  [--allowed-methods ALLOWED_METHODS] [--allowed-headers ALLOWED_HEADERS]\n                  [--api-key API_KEY]\n                  [--served-model-path SERVED_MODEL_PATH] \n                  [--served-model-name SERVED_MODEL_NAME]\n                  [--served-lpu-device-num SERVED_LPU_DEVICE_NUM]\n                  [--served-gpu-device-num SERVED_GPU_DEVICE_NUM]\n                  [--response-role RESPONSE_ROLE]\n                  [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE]\n                  [--root-path ROOT_PATH] [--verbose]\n</code></pre>"},{"location":"tgi_serve/#serving-model_1","title":"Serving Model","text":"<p>Below is an example of serving a HuggingFace model. The model to be served must be pre-compiled using the HyperDex Compiler SDK.</p> <pre><code>$ hdex-serve \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --served-model-path /opt/hyperdex/models \\\n  --served-model-name TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n  --served-lpu-device-num 1 \\\n  --served-gpu-device-num 1 \\\n  --verbose\n\nINFO:     Started server process [380461]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre> <p>Tip</p> <p>HyperDex-Serve can be used to start the server with the above command, and you can test it using the client code from TGI. Example code is available in <code>/opt/hyperdex/examples</code> for reference.</p>"},{"location":"tgi_serve/#descriptions-of-hyperdex-serve-arguments","title":"Descriptions of HyperDex-Serve Arguments","text":"Arguments Description <code>-h, --help</code> show help message and exit <code>--host HOST</code> host name <code>--port PORT</code> port number <code>--allow-credentials</code> allow credentials <code>--allowed-origins ALLOWED_ORIGINS</code> allowed origins <code>--allowed-methods ALLOWED_METHODS</code> allowed methods <code>--allowed-headers ALLOWED_HEADERS</code> allowed headers <code>--api-key API_KEY</code> If provided, the server will require this key to be presented in the header. <code>--served-model-path SERVED_MODEL_PATH</code> The path to model checkpoint used in the API. <code>--served-model-name SERVED_MODEL_NAME</code> The model name used in the API. If not specified, the model name will be the same as the huggingface name. <code>--served-lpu-device-num SERVED_LPU_DEVICE_NUM</code> The total number of LPU device used in the API <code>--served-gpu-device-num SERVED_GPU_DEVICE_NUM</code> The total number of GPU device used in the API <code>--response-role RESPONSE_ROLE</code> The role name to return if <code>request.add_generation_prompt=true</code>. <code>--ssl-keyfile SSL_KEYFILE</code> The file path to the SSL key file <code>--ssl-certfile SSL_CERTFILE</code> The file path to the SSL cert file <code>--root-path ROOT_PATH</code> FastAPI root_path when app is behind a path based routing proxy <code>--verbose</code> Lanch the program with verbose mode"},{"location":"trouble_shoot/","title":"Useful Commands","text":"<p>Warning</p> <p>Here are some methods that was shared with previous error cases. Based on the experiences of our previous PoC customers, most of the problems occur when pressing <code>Ctrl+C</code> while running the LPU\u2122. Please do not do this!</p>"},{"location":"trouble_shoot/#system-management-interface","title":"System Management Interface","text":"<p>You can check the status of the device that is running. This code shows online devices and each memory uses.</p> <pre><code>$ watch -n 1 hyperdex-smi\nFri Sep  6 17:17:52 2024\n+-----------------------------------------------------------------------------+\n| HYPERDEX-SMI            XRT Version: 2022.2     HyperDex Version: 1.3.2     |\n+-----------------------------------------------------------------------------+\n| FPGA Name        Persistence-M| Bus-Id     Bitstream | Volatile Uncorr. ECC |\n|      Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | FPGA-Util Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|    0      XILINX U55C    Off  | 0000:b1:00.1      On |                  N/A |\n|       48C    P8    34W / 225W |      0MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|    1      XILINX U55C    Off  | 0000:b2:00.1     Off |                  N/A |\n|       34C    P8    16W / 225W |      0MiB /     0MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|    2      XILINX U55C    Off  | 0000:ca:00.1     Off |                  N/A |\n|       34C    P8    16W / 225W |      0MiB /     0MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|    3      XILINX U55C    Off  | 0000:cb:00.1     Off |                  N/A |\n|       34C    P8    16W / 225W |      0MiB /     0MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>This refreshes the display every second. To make the refresh rate faster, decrease 1 to a lower number(e.g. 0.5). The number on the most left side is the id of the LPU\u2122.</p>"},{"location":"trouble_shoot/#reset-the-lpu-device","title":"Reset the LPU device","text":"<p>Instead of pressing <code>Ctrl+C</code>, please reset the device. First, check the device id with the hyperdex-smi. Next reset the device with the following code.</p> <pre><code>$ hyperdex-reset -d {LPU id}\n</code></pre>"},{"location":"vllm_api/","title":"vLLM API","text":"<p>HyperDex supports the vLLM framework to run on LPU. As you know, the vLLM framework officially supports a variety of hardware including GPU, TPU, and XPU. HyperDex has its own branch of vLLM with a backend specifically designed for LPU, making it very easy to use. If your system is already using vLLM, you can switch hardware from GPU to LPU without changing any code.</p>"},{"location":"vllm_api/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS, Rocky 8.4</li> <li>Python: 3.9 ~ 3.11</li> <li>torch: 2.4.0+cpu (in LPU only env) or 2.4.0+cu121 (in LPU+GPU env)</li> <li>Xilinx Runtime Library</li> <li>HyperDex Runtime &amp; Compiler stack</li> </ul>"},{"location":"vllm_api/#install-with-pip","title":"Install with pip","text":"<p>You can install <code>hyperdex-vllm</code> using pip, which requires access rights to HyperAccel's private PyPI server. To install the HyperDex Python package, run the following command:</p> <pre><code>$ # (Recommended) Create a new conda environemnt.\n$ conda create -n hdex-env python=3.10 -y\n$ conda activate hdex-env\n\n$ # Install HyperDex-vLLM in LPU only env\n$ pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cpu\n$ pip install -i https://pypi.hyperaccel.ai/simple vllm==0.6.1+cpu\n\n$ # Install HyperDex-vLLM in LPU+GPU env\n$ pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n$ pip install -i https://pypi.hyperaccel.ai/simple vllm==0.6.1+cu121\n\n$ # Install HyperDex-vLLM source code\n$ git clone git@github.com:Hyper-Accel/vllm.git\n</code></pre>"},{"location":"vllm_api/#text-generation-with-hyperaccel-lputm","title":"Text Generation with HyperAccel LPU\u2122","text":"<p>HyperDex-vLLM generates tokens very similar to vLLM's <code>generate</code> function, enabling you to easily generate tokens as demonstrated in the example below. Ensure that device=\"fpga\", and num_lpu_devices=1 are set.</p> <pre><code># You can see this file in our vLLM repo. (vllm/examples/lpu_inference.py)\n\nfrom vllm import LLM, SamplingParams\n\n# Create prompts and a sampling params object.\nprompts = [\"Hello, my name is\"]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, top_k=1, min_tokens=30, max_tokens=30)\n\n# Create an LLM\nllm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device=\"fpga\", num_lpu_devices=1)\n\n# Generate texts from the prompts. \noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n</code></pre>"},{"location":"vllm_api/#gpu-lpu-hybrid-system","title":"GPU-LPU Hybrid System","text":"<p>HyperDex supports a heterogeneous GPU-LPU hardware system for executing large language models (LLM). Each hardware type offers distinct strengths: GPU excels in large-scale parallel computations, while LPU is designed to fully optimize memory bandwidth utilization. </p> <p>Since the prefill stage is compute-bound and the decode stage is memory-bound, the hybrid system processes prefill stage using a GPU, and decode stage using a LPU. This approach sighificantly boosts LLM performance!</p> <p>To enable the hybrid system, simply add the option num_gpu_devices=1.</p> <pre><code># Create an LLM.\nllm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device=\"fpga\", num_lpu_devices=1, **num_gpu_devices=1**)\n</code></pre>"},{"location":"vllm_api/#option-for-sampling-params","title":"Option for Sampling Params","text":"<p>Sampling Params refer to setting that control how a model generates text. vLLM supports various sampling params to support various features. However, due to the current limitations of the LPU, only the sampling parameters listed below are supported. We are planning to extend and update our coverage.</p> Sampling Arguments Description <code>max_tokens</code> The number of tokens to be generated. Default value is <code>16</code> <code>top_p</code> Top-P sampling. Default value is <code>0.7</code> <code>top_k</code> Top-K sampling. Default value is <code>1</code> <code>temperature</code> Smoothing the logit distribution. Defualt value is <code>1.0</code> <code>repetition_penalty</code> Give penlaty to logits. Default value is <code>1.2</code> <code>stop</code> Token ID that signals the end of generation. Default value is <code>eos_token_id</code>"},{"location":"vllm_api/#option-for-llm-engine","title":"Option for LLM Engine","text":"<p>LLM Engine is a core component of vLLM because it performs various functions. For example, it initializes hardware, manages hardware resources, and schedules requests.</p> LLM Engine Arguments Description <code>model</code> Name of path of the huggingface model to use. Default: \"facebook/opt-125m\" <code>device</code> Device type for vLLM execution. Default: \"cuda\" <code>num_lpu_devices</code> Number of LPU to compute in parallel. Default: 1 <code>num_gpu_devices</code> Number of GPU to compute in parallel. Default: 0 <code>tokenizer</code> Name of path of the huggingface tokenizer to use. If not specified, model name of path will be used <code>trust-remote-code</code> Trust remote code from huggingface. Default: False"},{"location":"vllm_serve/","title":"vLLM Server","text":""},{"location":"vllm_serve/#requirements-and-install-guide","title":"Requirements and Install Guide","text":"<p>Requirements and Install Guide is same as vLLM API. </p>"},{"location":"vllm_serve/#serving-model","title":"Serving Model","text":"<p>hyperdex-vllm provides an HTTP server that implements vLLM API.  You can execute the server by the command below.</p> <pre><code>$ python -m vllm.entrypoints.api_server --model facebook/opt-1.3b \\\n        --device fpga --num_lpu_devices 1\n\n... OMISSION ...\n\nINFO:     Started server process [27157]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"vllm_serve/#descriptions-of-hyperdex-vllm-serve-arguments","title":"Descriptions of HyperDex-vLLM Serve Arguments","text":"<p>Arguments are the same as vLLM Engine.</p>"},{"location":"vllm_serve/#client","title":"Client","text":"<p>To call the server, you can use the client example provided in <code>vllm/examples</code> ensuring that <code>use_beam_search=False</code>.</p> <pre><code># You can see this file in our vLLM repo. (vllm/examples/lpu_client.py)\npython lpu_client.py --stream\n</code></pre>"},{"location":"vllm_serve/#openai-compatible-server","title":"OpenAI Compatible Server","text":""},{"location":"vllm_serve/#serving-model_1","title":"Serving Model","text":"<p>hyperdex-vllm also provides an HTTP server that implements OpenAI's Completions API. You can call the server by the command below. <pre><code>python -m vllm.entrypoints.openai.api_server --model facebook/opt-1.3b \\\n        --device fpga --tensor-parallel-size 1\n\n... OMISSION ...\n\nINFO:     Started server process [27157]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre></p>"},{"location":"vllm_serve/#client_1","title":"Client","text":"<p>To call the server, you can use OpenAI Python client library, or any other HTTP client. Change stream option if you want.</p> <pre><code># You can see this file in our vLLM repo. (vllm/examples/lpu_openai_client.py)\npython lpu_openai_client.py\n</code></pre>"}]}